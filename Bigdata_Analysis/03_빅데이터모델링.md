# 3. 빅데이터 모델링
## 1. 분석 모형 설계
#### 분석 방법
- **통계 분석**(Statistical analysis) : 특정 집단이나 불확실한 현상을 데이터를 통해 이해하고 추론을 통해 의사결정하는 과정
  - 기술 통계 : 데이터를 요약/정리하고 이해하기 위해 평균, 표준편차 등 기초통계량을 구하거나 그래프로 표현하는 분석방식
  - 추론 통계 : 수집된 데이터를 기반으로 모집단에 대해 추정하고 가설을 검정하는 분석 방법
- **데이터 마이닝**(Data Mining) : 데이터에 숨어있는 유용한 정보를 찾아내는 과정. 분류 분석, 추정 분석, 예측 분석, 연관 분석, 군집 분석, 기술 분석 등이 존재
- **머신 러닝**(Machine Learning) : 분석 모형 알고리즘이 데이터를 학습하고 학습한 정보를 바탕으로 결과를 출력하는 분석방법. 종속변수의 존재 여부, 학습 방법 등에 따라 지도 학습, 비지도 학습, 강화학습으로 구분함
  - **지도 학습**(Supervised Learning) : 정답에 해당하는 종속변수가 포함되어 있는 데이터를 학습. 종속변수와 독립변수 간의 관계를 분석하여 분류, 예측 등의 문제를 해결함. 종속변수가 연속형인 경우 수치 예측, 범주형인 경우 분류 예측을 시행한다.
    - **지도 학습의 종류** : 회귀 분석, 로지스틱 회귀분석, 나이브 베이즈, KNN, 의사결정나무, 인공신경망, SVM, 랜덤포레스트
  - **비지도 학습**(Unsupervised Learning) : 종속변수가 포함되지 않는 데이터를 학습. 예측 문제보다는 현상 설명, 특징 도출, 패턴 도출 등의 문제를 해결한다.
    - **비지도 학습의 종류** : 군집화(K-means, SOM, 계층군집), 차원 축소(주성분분석, 선형판별분석), 연관분석, 자율학습 인공신경망


#### 데이터 유형에 따른 모형 구분
|데이터 유형|연속형 종속변수|범주형 종속변수|종속변수 없음|
|----|----|----|--------|
|연속형 독립변수|회귀분석 트리 모형,인공신경망,SVR,KNN|로지스틱 회귀분석 트리 모형, 인공신경망, SVM, KNN|주성분 분석, 군집 분석|
|범주형 독립변수|회귀분석, t-test, ANOVA, 트리 모형, 인공신경망|로지스틱 회귀분석, 카이제곱 검정, 트리 모형, 인공신경망, 나이브베이즈|연관 분석, 판별분석|
|연속형+범주형 독립변수|회귀분석, 트리 모형, 인공신경망|트리 모형, 인공신경망|상관 분석|


#### 변수 선택
- 전진 선택법 : 상관관계가 큰 변수부터 순차적으로 모형에 추가하며 변수를 추가하는 방법
- 후진 제거법 : 모든 독립변수를 추가한 모형에서 상관관계가 작은 변수부터 순차적으로 제거하는 방법
- 단계적 선택법 : 전진 선택법으로 순차적으로 변수를 추가하면서 중요도가 약해진 변수를 후진 제거법으로 제거하는 방법


#### 파라미터와 하이퍼 파라미터
- 파라미터 : 모형 내부 요소로 모형의 성능에 직접적인 영향을 미친다. 모형이 데이터를 학습한 결과 값으로 자동으로 결정된다.
- **하이퍼 파라미터** : 모형 외부 요소로 모형의 성능에 간접적인 영향을 미친다. 사용자가 설정하는 값으로 학습 과정에 영향을 주고 학습 결과인 파라미터 값에 영향을 준다.
- 하이퍼 파라미터 튜닝 방법
  - **매뉴얼 서치**(Manual Search) : 사용자가 직감 또는 경험에 근거하여 직접 하이퍼 파라미터를 조합하고 조정하는 방법. 매우 비효율적인 방법
  - **그리드 서치**(Grid Search) : 가능한 모든 조합을 시도하여 최적의 파라미터 값을 찾는 방법. 후보를 직접 선정하므로 후보 내에서 가장 좋은 결과를 얻을 수 있으나 후보가 아닌 값은 시도하지 않는다.
  - **랜덤 서치**(Random Search) : 하이퍼 파라미터의 값 범위를 지정하고 무작위 표본추출을 통해 생성한 조합을 시도하여 최적의 파라미터 값을 찾는 방법. 그리드 서치와 거의 동일하나 그리드 서치의 단점을 보완한다. **난수를 통해 확률적으로 탐색하므로 불필요한 값의 중복을 없앤다는 특징**을 가진다.


#### 데이터 분할
- **홀드아웃**(Hold-Out) : 가장 보편적인 방법으로 랜덤 추출을 통해 데이터를 분할함. 일반적으로 학습 데이터와 검증 데이터를 60-80%, 테스트 데이터를 20-40%로 분할한다.
- **K-fold 교차 검증** : 테스트 데이터를 제외한 데이터를 무작위로 중복되지 않는 K개의 데이터로 분할한다. (K-1)개의 데이터를 학습 데이터로 사용하고 나머지 1개 데이터를 검증 데이터로 사용한다. 이후 검증 데이터를 바꾸며 K번 반복하여 분할된 데이터가 한 번씩 검증 데이터로 사용된다.
- **부트스트랩**(Bootstrap) : 데이터의 분포가 치우쳐 있거나 데이터 건수가 너무 적을 때 사용 가능한 기법이다. 복원 추출을 통해 전체 데이터와 동일한 사이즈의 샘플 데이터를 추출한다. 어떤 데이터는 부트스트랩 샘플에 한 번 이상 포함되나 어떤 데이터는 한 번도 포함되지 않을 수 있다. 데이터의 사이즈가 충분히 크다면 전체 데이터의 약 63.2%를 포함한다.
- 데이터가 부족한 경우 검증용 데이터는 분할하지 않기도 한다.


#### 분석 모형 구축의 절차
- 요건 정의-모델링-검증 및 테스트-적용 단계로 구성
1. **요건 정의** : 기획 단계에서 도출한 내용을 요건 정의로 구체화하는 단계. **요구사항 도출-분석 추진 계획 수립-요구사항 확정**으로 이루어짐
2. **모델링** : 정의된 요건에 따라 본격적인 분석을 수행하는 단계. 데이터 준비 및 탐색적 데이터 분석을 수행하고 모델링과 성능 평가를 반복 수행하여 최종 모형을 선정한다. **데이터 마트 설계 및 구축-탐색적 분석 및 유의변수 도출-모델링-모델 성능 평가**로 이루어짐
3. **검증 및 테스트** : 분석 모형을 가상 운영 환경에서 테스트하는 단계. **운영 환경 테스트-비즈니스 영향도 평가**로 이루어짐
4. **적용** : 분석 결과를 실제 운영 환경에 적용하는 단계. **운영 시스템 적용-주기적 모델 업데이트**로 이루어짐


## 2. 분석 기법 적용
#### 회귀분석의 가정 - 기출
- **선형성** : 독립변수와 종속변수는 선형적. 종속변수는 독립변수와 회귀계수의 선형적 조합으로 표현 가능. 산점도를 통해 선형성을 확인 가능
- **독립성** : 단순 회귀분석에는 잔차와 독립변수의 값이 서로 독립. 다중 회귀분석에서는 독립변수 간에 상관성 없이 독립
- **등분산성** : 잔차의 분산이 독립변수와 무관하게 일정. 잔차가 고르게 분포해야 함
- **정규성** : 잔차항이 정규분포의 형태를 띰. 잔차항의 평균은 0이고 분산이 일정함.

회귀분석의 위의 네 가정을 만족하는 데이터의 경우에 사용한다.


#### 회귀분석의 종류
- 단순 회귀 : 독립변수가 1개이며 종속변수와의 관계가 직선
- 다중 회귀 : 독립변수가 k개이며 종속변수와의 관계가 선형
- 다항 회귀 : 독립변수와 종속변수와의 관계가 1차 함수 이상인 관계
- 곡선 회귀 : 독립변수가 1개이며, 종속변수와의 관계가 곡선
- 비선형 회귀 : 회귀식의 모양이 미지의 모수들의 선형관계로 이뤄져있지 않은 모형


#### 단순선형 회귀분석
- 독립변수와 종속변수가 한 개씩 있으며 오차항이 있는 선형관계로 이뤄진다.(직선관계)
- 회귀계수는 최소제곱법을 사용하여 추정한다.
- 결정계수(R의 제곱)는 회귀 모형의 설명력을 보여주는 지표이며 회귀선의 정확도를 평가한다.
  - R의 제곱=회귀제곱합/전체제곱합


#### 다중선형 회귀분석
- 독립변수가 k개인 경우. 독립변수와 종속변수와의 관계는 1차 함수 이상인 경우이며 선형이다.
- 모형의 통계적 유의성은 F-통계량으로 확인하며, p-value가 0.05보다 작으면 회귀식이 통계적으로 유의하다고 본다.
- 다중선형 회귀분석의 검정
  - 회귀계수의 유의성 : 회귀계수의 유의성은 t-통계량을 통해 확인
  - 결정계수(R제곱) : 회귀 모형의 설명력을 보여주는 지표
  - 모형의 적합성 : 잔차와 종속변수의 산점도로 확인
  - 다중공선성 : 설명 변수들 사이에 선형관계가 존재하여 회귀계수의 추정에 부정적인 영향을 미치는 것을 의미


#### 규제가 있는 회귀 분석 - 기출
- **릿지 회귀**(Ridge)
  - 높은 상관관계가 있는 변수 간 검청 오차(또는 검정 MSE)가 최소인 모델을 찾는 것을 목적으로 가진다.
  - 규제항을 비용 함수에 추가하며, 모델의 훈련이 끝나면 모델의 성능을 규제 없는 성능 지표로 평가한다.
  - 알파는 모델을 얼마나 많이 규제할지 조절하는 것으로, 값이 커질수록 모든 가중치가 0에 수렴한다.
  - L2 규제 : 모든 파라미터 제곱만큼의 크기를 규제하는 방식. 가중치를 제약하여 가중치 값을 널리 퍼지도록 하는 효과를 준다.

- **라쏘 회귀**(Lasso)
  - 변수 선택을 통해 변수 간 검정오차(또는 검정 MSE)가 최소인 모델을 찾는 것을 목적으로 가진다.
  - 릿지와 마찬가지로 규제항을 추가하지만 제곱이 아닌 절댓값을 적용한 값임
  - 알파의 값이 설정되면 중요하지 않은 변수들의 가중치가 0이 되어 제거되기에 해당 변수가 없는 것과 마찬가지가 된다.
  - L1 규제 : 가중치 벡터를 0으로 규제하는 방식. 의미있는 변수만을 선택하는 효과를 준다.
  - 다중공선성이 있는 경우에는 다중공선성이 발생하는 변수 그룹의 모든 변수가 제거되는 경우가 발생할 수 있어 릿지 회귀보다 성능이 떨어질 수 있다. 

- **엘라스틱넷 회귀**(Elastic Net)
  - 릿지와 라쏘 회귀의 절충안. 두 회귀의 규제항을 단순히 더하여 사용하며 혼합 비율을 조절하여 어느 방식의 비중을 크기 할 것인지 결정한다.


#### 로지스틱 회귀분석 - 개념 기출
- **독립변수의 선형결합을 이용해 사건의 발생 여부를 예측하며, 종속변수가 범주형일 경우에 사용하는 회귀분석**
- **종속변수의 범주가 두 개일 때 이항 로지스틱 회귀분석이라 하고, 그 이상이면 다항 로지스틱 회귀분석이라 함**
- **일반적인 선형 회귀분석은 x값과 y값 모두 연속적인 값을 가진다. 하지만 로지스틱 회귀분석의 경우,  y값을 0~1(확률 P) 사이의 값을 갖게 하고 두 가지고 분류하려고 하는 과정이므로 수식을 변환하는 과정이 필요하다.**
- **선형 회귀분석은 정규분포를 따르지만, 로지스틱 회귀분석은 이항분포를 따른다는 차이점이 있다.**
- 모형 적합성
  - 모형의 유의성 : 모형이 설명하지 못하는 데이터의 정도를 의미하는 Deviance(이탈도)를 통해 검증. 이탈도가 적을수록 유리함
  - 계수의 유의성 : 왈드(ward) 검정을 통해 독립변수가 종속변수에 미치는 영향 확인. 검정 통계량인 z-value의 p-value가 유의수준보다 작으면 계수가 유의함
  - 모형의 설명력 : 로지스틱 회귀분석은 보통 결정계수가 낮게 나오는 편이므로, McFadden이 제안한 의사결정계수를 사용하는 것이 일반적임. AIC값이 작을수록 설명력이 좋음


#### 의사결정나무 분석
- 과거에 수집된 자료를 분석해서 이들 사이에 존재하는 패턴을 나타내는 분류모형을 나무모형으로 나타낸 것. 전체 자료를 여러 개의 소집단으로 분류하거나 예측하는 데 사용되는 기법
- 분석 과정
  - 의사결정 나무의 성장 : 데이터의 구조에 따라 분리 기준과 정지 규칙을 설정한다.
  - 정지 규칙 : 더 이상 트리가 분리되지 않도록 하는 규칙을 설정한다.
  - 가지치기 : 불필요한 가지를 제거하여 모형의 복잡도를 줄이는 과정
- 장점
  - 해석의 용이성 : 나무 구조로 표현되어 사용자의 이해가 쉬움
  - 상호작용 효과의 해석 가능 : 두 개 이상의 변수의 영향 정도를 쉽게 파악
  - 비모수적 모형 : 선형성, 정규성, 등분산성 등의 가정을 필요로 하지 않는 비모수적인 방법. 이상값에 민감하지 않음
  - 유연성, 정확도 높음 : 대용량 데이터에서도 빠르게 생성할 수 있다.
- 단점
  - 비연속성 : 연속형 변수를 비연속적 값으로 취급하여 분리 경계점에서는 예측오류가 커짐
  - 선형성 결여 : 각 변수의 고유한 영향력을 해석하기 어려움
  - 비안정성 : 학습용 자료에 의존하여 과대 적합 발생 가능성이 높음



#### 인공신경망 분석
- 사람 두뇌의 신경세포인 뉴런이 전기신호를 전달하는 모습을 모방한 기계학습 모델. 간단한 계산능력을 가진 처리 단위인 뉴런/노드들이 복잡하게 연결된 구조를 이루고 있으며, 입력데이터를 기초로 가중치를 통해 의사결정을 함
- 인공신경망의 구조
  - **활성 함수** : 노드에 입력된 값을 비선형 함수에 통과시켜 다음 노드로 전달하는데, 이 비선형 함수를 활성 함수라 함
    - **Sigmoid 함수** : 로지스틱 함수라고도 함. 곡선의 형태로 0과 1사이의 값을 출력. 은닉층을 거칠 때마다 값이 0으로 수렴하는 문제를 가짐
    - **ReLU 함수** : 입력값이 0보다 작으면 0을, 0보다 크면 입력값을 그대로 출력하는 함수. 연산이 빠르지만 0보다 작은 값에 대해 뉴런이 작동하지 않을 수 있음
    - **Tanh 함수** : Sigmoid의 확장 형태. -1과 1 사이의 값을 출력하며 Sigmoid보다 학습속도가 빠름
- 인공신경망은 **입력층, 은닉층, 출력층**의 세 가지 층으로 구성됨
  - 입력층은 예측을 위한 데이터를 입력받는다.
  - 은닉층은 입력층으로부터 전달받은 값을 이용하여 가중 합과 편향을 계산하고, 활성 함수에 적용하여 결과를 산출함
  - 출력층은 활성 함수의 결과를 담고 있으며, 출력 범주의 수가 같도록 구성됨
- 역전파 알고리즘 : 인공신경망을 학습시키기 위한 일반적인 알고리즘. 출력값으로 결정된 결과값의 오차를 역으로 입력층으로 전파하면서 오차가 최소가 될 수 있도록 가중하는 과정. 입력층에서 차례대로 가중치를 계산하는 것보다 빠르고 정확함
- 인공신경망의 종류
  - 단층 퍼셉트론 : AND, OR 연산이 가능하지만, XOR은 선형 분리할 수 없는 문제점이 있음
  - 다층 퍼셉트론 : 입력층과 출력층 사이에 하나 이상의 은닉층을 추가해 비선형 데이터에 대해 학습할 수 있도록 한 퍼셉트론 구조. 은닉층을 가지며 역전파 알고리즘을 통해 다층으로 만들어졌고, 활성화 함수로 시그모이드를 사용한다.
    - 문제점으로 과적합과 기울기 소실이라는 문제점을 가진다.
- 장점 : 스스로 가중치를 학습하여 다양하고 많은 데이터에 효과적임. 패턴인식, 분류, 예측에 효과적임, 비선형 문제 해결
- 단점 : 복잡한 모형일수록 학습 시간이 오래걸림, 추정한 가중치의 신뢰도가 낮음, 결과 해석의 어려움, 은닉층와 은닉 노드 수 결정이 어려움


#### 서포트 벡터 머신(SVM : Support Vector Machine) - 특징 기출
- 데이터를 분리하는 초평면 층에서 데이터들과 가장 거리가 먼 초평면을 분리하는 지도 학습 기반의 이진 선형 분류 모델
- 사물 인식, 패턴 인식, 손글시 숫자 인식 등의 다양한 분야에서 활용됨
- 비확률적 선형 판별에 기초한 이진 분류기
- **특징**
  - **공간상에서 최적의 분리 초평면을 찾아 분류와 회귀를 수행함**
  - **변수 속성 간의 의존성을 고려하지 않으며, 모든 속성을 활용함**
  - **훈련 시간이 느린 편이지만, 정확성이 높고 과적합 가능성이 작다.**
- 종류
  - 하드 마진 SVM : 마진의 안쪽이나 바깥쪽에 잘못 분류된 오분류를 절대 허용하지 않음
  - 소프트 마진 SVM : 잘못 분류된 오분류를 허용함
- 구성요소
  - 결정경계 : 데이터 분류의 기준이 되는 경계
  - 초평면 : n파원 공간의 (n-1) 차원 평면
  - 마진 : 결정경계에서 서포트벡터까지의 거리
  - 서포트벡터 : 결정경계와 가장 가까이에 있는 학습 데이터들의 집합
  - 슬랙 변수 : 완벽한 분리가 불가능할 때 허용된 오차를 위한 변수
- **커널 트릭 : 선형 분류가 불가능한 데이터를 처리하기 위해 데이터의 차원을 증가시켜 하나의 초평면을 분리가 가능하도록 도와주는 커널 함수를 사용하는 것**


#### 연관성 분석
- 데이터에 존재하는 항목 간 상호 관계와 종속관계를 찾아내는 분석 기법
- 장바구니 분석, 서열 분석이라고도 함. 콘텐츠 기반 추천의 기본 기법
- 특징
  - 목적 변수가 없어 분석 방향과 목적이 없어도 적용할 수 있음
  - 조건 반응으로 표현되어 결과를 해석하기 쉬움
  - 너무 세분된 품목은 의미없는 결과를 도출할 수 있음
- 주요 용어
  - 지지도 : 전체 거래 중 항목 A와 B를 동시에 포함하는 거래의 비율
  - 신뢰도 : A 상품을 샀을 때 상품을 살 조건부 확률에 대한 척도
  - 향상도 : 규칙이 우연에 의해 발생한 것인지를 판단하기 위해 연관성의 정도를 측정하는 척도


#### 군집 분석
- 관측된 여러 개의 변수 값에서 유사성에만 기초하여 n개의 군집으로 집단화한 뒤, 그 집단의 특성을 분석하는 다변량 분석 기법
- 계층적 군집
  - 유사한 개체를 군집화하는 과정을 반복하여 군집을 형성한 것
  - 병합적 방법 : 작은 군집으로부터 시작하여 군집을 병합.
  - 분할적 방법 : 큰 군집에서 군집을 분리해나가는 과정
- 계층도(덴드로그램) : 군집의 결과를 보여주는 그림. 각 개체는 단 하나의 군집에만 속함
- 군집간 거리 측정 방법
  - 최단연결법 : 각 군집에서 하나씩 관측값을 뽑았을 때 나올 수 있는 최솟값을 두 군집 사이의 거리로 측정
  - 최장연결법 : 각 군집에서 하나씩 관측값을 뽑았을 때 나올 수 있는 최댓값을 두 군집 사이의 거리로 측정
  - 중심연결법 : 두 군집 중심 간의 거리를 측정
  - 평균연결법 : 모든 항목에 대한 거리 평균
  - 와드연결법 : 군집 내의 오차 제곱합에 기초하여 군집을 수행
- 군집간 거리 계산 방법
  - 유클리드 거리 : 두 점 간 차를 제곱하여 모두 더한 값
  - 맨해튼 거리 : 두 점 간 차의 절대값을 합한 값
  - 민코프스키 거리 : m차원 민코프스키 공간에서의 거리(1일 경우 맨해튼, 2일 경우 유클리드)
  - 표준화 거리 : 변수의 측정단위를 표준화한 거리
  - 마할라노비스 거리 : 변수의 표준화와 함께 변수 간의 상관성을 동시에 고려한 통계적 거리


#### 군집 분석-K 평균 군집 - 기출
- 주어진 데이터를 **k개의 군집으로 묶는 알고리즘.** 초기 값으로 k개의 군집으로 지정하고, 각 개체를 가까운 초기 값에 할당하여 군집을 형성함
- 절차
  1. 군집의 수 k를 임의로 선택
  2. 데이터를 가장 가까운 군집 중심에 할당
  3. 각 군집 내의 자료들의 평균을 계산하여 군집의 중심을 갱신
  4. 군집 중심의 변화가 거의 없을 때까지 단계 2와 단계 3을 반복 진행


#### 군집 분석-혼합 분포 군집
- 데이터가 k개의 모수적 모형의 가중 합으로 표현되는 모집단 모형으로부터 나왔다는 가정하에 자료로부터 모수와 가중치를 추정하는 방법
- k개의 각 모형은 군집을 의미하며, 각 데이터는 k개의 군집 중 어느 군집에서 나왔을 확률이 높은지에 따라 군집의 분류가 이루어진다.
- EM 알고리즘
  - 관측되지 않은 잠재변수에 의존하는 확률뫄델에서 최대 가능도나 최대 사후 확률을 갖는 모수의 추정값을 찾는 반복적인 알고리즘
  - E 단계와 M 단계로 나누어 진행함
  - E 단계에서 잠재변수 Z의 기대치를 계산하고 M 단계에서 기대치를 활용하여 파라미터를 추정함
- 특징
  - 확률 분포를 도입하여 군집을 수행함
  - 군집을 몇 개의 모수로 표현할 수 있고, 서로 다른 크기의 군집을 찾을 수 있다.
  - EM 알고리즘을 활용한 모수 추정에서 데이터가 커지면 시간이 오래 걸린다.
  - 군집이 너무 작으면 추정이 어려워짐
  - 이상치에 민감하여 사전에 이상치를 제거하는 작업이 필요함


#### 군집 분석-자기 조직화 지도(SOM : Self-Organizing Maps)
- 인공신경망 개념. 자율 학습 방법에 따른 군집화를 적용한 알고리즘
- 고차원의 데이터를 저차원의 뉴런으로 정렬하여 지도의 형태로 형상화하는 비지도 신경망
- 구성
  - 입력층 : 입력 벡터를 받는 층. 입력변수의 개수와 같은 뉴런이 존재
  - 경쟁층 : 각각의 뉴런이 입력 벡터와 얼마나 가까운지 계산하여 연결 강도를 재조정 학습
- 알고리즘
  1. SOM 맵의 노드에 대한 연결 강도 초기화
  2. 입력 벡터 제시
  3. 유클리드 거리를 사용하여 입력 벡터와 프로토타입 벡터의 유사도를 계산
  4. 입력 벡터와 가장 거리가 짧은 프로토타입 벡터 탐색
  5. BMU와 그 이웃들의 연결 강도 재조정. 이후 단계 2로 가 반복


#### 고급 분석 기법-범주형 자료 분석 - 기출
- 범주 또는 집단으로 나누어진 자료. 범주형 자료의 순서가 없으면 명목형 자료, 순서가 있으면 순서형 자료라고 함

|독립변수|종속변수|분석 방법|
|----|----|----|
|범주형|연속형|t-검정, 분산분석(ANOVA)|
|범주형|범주형|분할표 분석, 카이제곱 검정, 피셔의 정확도 검정|
|연속형|범주형|로지스틱 회귀분석|


#### t-검정
- 두 집단 간의 평균을 비교하는 모수적 통계방법. t분포를 이용해 가설을 검정하며 정규성, 등분산성, 독립성을 가정한다.
- 단일표본, 독립표본, 대응표본에 대한 검정을 수행함
  - 단일표본 검정 : 표본의 평균으로 모집단의 평균을 검정
  - 독립표본 검정 : 서로 다른 두 집단의 평균의 차이를 검정
  - 대응표본 검정 : 동일한 집단의 사전 사후 차이를 검정


#### 분산분석(ANOVA : Analysis of Variance)
- 둘 이상의 집단의 평균을 비교하는 모수적 통계방법. F분포를 이용해 가설을 검정하며 정규성, 등분산성, 독립성을 가정함
- 일원분산분석(one-way ANOVA) : 범주형 변수가 한 개인 경우 사용
  - 귀무가설 : 모든 집단의 평균이 같다. / 대립가설 : 하나 이상의 집단 평균이 다르다.
- 이원분산분석(two-way ANOVA) : 범주형 변수가 두 개 이상인 경우 두 변수의 상호작용효과와 각 변수의 주효과를 분석함
  - 귀무가설 : 두 변수는 상호작용효과가 없다 / 대립가설 : 두 변수는 상호작용효과가 있다.


#### 분할표 분석
- 두 범주형 변수의 빈도 분포표를 작성할여 변수 간 상호 관련성을 분석하는 방법
- 상대위험도(RR : Relative Risk) : 두 집단이 사건발생 확률의 비
- 승산비(OR : Odds Ratio) : 특정 집단에 대한 사건발생 확률과 사건이 발생할지 않을 확률의 비
- 교차비(OR) : odds(A)/odds(B)=ad/bc

|상대위험도|교차비|해석|
|----|----|----|
|RR<1|OR<1|A 집단의 사건 발생 확률이 낮음|
|RR=1|OR=1|집단과 사건 발생 확률은 연관성이 없음|
|RR>1|OR>1|A 집단의 사건 발생확률이 높음|
- 활용 : 집단에 따라 사건 발생 가능성을 확인하는 방법
  - 코호트 연구 : 특정 집단을 대상으로 선정하고 장기간의 추적을 통해 미리 조사한 위험인자들과 질병 발생 간의 연관성을 연구
  - 환자-대조군 연구 : 질병이 있는 환자군과 없는 대조군을 따로 선정하고 위험인자를 나중에 조사함


#### 카이제곱 검정
- 범주형 자료 간의 차이를 분석하는 모수적 통계방법. x^2 분포를 이용하여 적합성 검정, 독립성 검정, 동질성 검정으로 나뉨
- 적합도 검정 : 하나의 범주형 변수에 대하여 데이터가 특정 분포를 만족하는지 검정함
  - 귀무가설 : 분포가 기대 분포와 같다. / 대립가설 : 분포가 기대 분포와 같지 않다.
- 동질성 검정 : 서로 다른 집단에 대한 범주형 변수의 분포가 동질인지 검정한다.
  - 귀무가설 : 두 집단의 분포가 같다. / 대립가설 : 두 집단의 분포가 같지 않다.
- 독립성 검정 : 두 범주형 변수가 서로 독립적인지 영향을 미치는지 검정한다.
  - 귀무가설 : 두 변수는 연관성이 없다. / 대립가설 : 두 변수는 연관성이 있다.


#### 피셔의 정확 검정(Fisher's Exact Test)
- 가능한 모든 경우의 수를 직접 확인하는 검정 방법. 초기하 분포를 기반으로 함.
- 가설은 카이제곱 검정과 동일하며, 기대빈도가 5보다 작은 셀이 20%를 넘으면 카이제곱 검정보다 피셔의 정확 검정을 사용함
- ex: 분할표에서 표본의 수가 작거나 범주가 많아서 빈도수가 극도로 작은 경우 사용한다.


#### 고급 분석 기법-다변량 분석
- 여러 변수를 동시에 분석할 수 있는 모든 분석 방법을 가리킴
- 각 변수를 개별적으로 분석하지 않고 변수 간으 상관관계를 고려한다

#### 상관관계분석
- 변수들 간의 상관성을 분석한다.
- 피어슨 상관계수 : 두 변수의 공분산을 표준편차의 곱으로 나눈 값. 비선형 관계는 측정하지 못함
- 스피어만 상관계수 : 두 변수를 순위로 변환하여 순위의 상관계수로 비선형적인 관계를 나타낼 수 있음


#### 다차원 척도법(MDS : Multidimensional Scaling) - 기출
- **차원 축소를 통해 개체들 간의 관계를 상대적 위치로 시각화하여 나타내는 분석 방법.**
- 데이터가 **연속형 변수**인 경우 **거리 행렬을 이용한 다차원 척도법**을 사용한다.
- 데이터가 **순서형 척도**인 경우 순서척도를 거리로 변환하는 **비계량적 다차원 척도법**을 사용한다 
- 적용 절차
  1. 유클리드 거리행렬 등을 활용해 개체들 간의 유사성을 측정한다.
  2. 2차원 또는 3차원 공간에 개체를 점으로 배열한다 
  3. 스트레스 값을 부적합도로 측정하여 최소가 되도록 좌표를 조정한다.
    - 0-0.1 : 매우 좋음, 0.1-0.2 : 좋음, 0.2 이상 : 나쁨


#### 다변량 분산분석(MAMOVA : Multivariate Analysis of Variance)
- 2개 이상의 종속변수에 대한 분산분석 방법. 범주형 독립변수에 대한 평균벡터 차이를 분석함
- 종속변수 간에 서로 상관관계가 있는 경우 결합된 차이를 확인할 수 있다.
- 상관관계가 없는 경우 개별로 분산분석을 수행해야 한다. 정규성, 등분산성, 독립성을 가정한다.
  - 귀무가설 : 모든 집단의 평균벡터가 같다. / 대립가설 : 하나 이상의 집단의 평균벡터가 다르다


#### 주성분분석(PCA : Principal Component Analysis)
- 데이터 전체의 변동을 최대한 보존하는 주성분을 생성하는 차원축소 방법
- 주성분 : 데이터의 분산을 설명하는 설명변수들의 선형 결합으로 표현된다. 항상 설명변수와 동일한 수만큼 성분을 추출할 수 있다. 상관성이 적은 주성분을 활용하여 회귀분석의 다중공선성 문제를 해결할 수 있다.
- 누적 기여율(Cumulative Proportion) : 주성분을 고유값의 내림차순으로 정렬하여 상위 n개의 주성분으로 설명할 수 있는 정보량의 비율


#### 요인분석(FA : Factor Analysis)
- 변수들의 상관관계를 기반으로 공통의 요인을 찾아 데이터를 요약하고 차원을 축소하는 분석 방법
- 연역적 방법인 확인적 요인분석(CFA)와 귀납적 방법인 탐색적 요인분석(EFA)이 있다.
- 요인 추출 방법 : 주성분 분석과 공통요인법(Common Factor Analysis)이 많이 사용된다. 요인 수를 최소화하는 경우 주성분 분석을 선택한다.
- 요인 회전 방법 : 베리맥스, 쿼티멕스, 이쿼멕스 등의 직각회전 방법과 오블리민 등의 사각 회전으로 나뉜다. 베리멕스 방법을 주로 사용한다.

#### 주성분분석과 요인분석의 비교
- 공통점 : 차원 축소 기능, 다른 분석을 위한 사전 분석
- 차이점
  - PCA :선형적 결합 중심, 데이터를 요약하는 주성분을 추출함, 주성분간 중요도 차이 있음
  - FA : 잠재적 결합 중심, 상관성 기준 잠재 변수를 생성함, 새로운 변수들은 서로 대등함


#### 판별분석(Discriminant Function Analysis)
- 연속형 독립변수들의 선형조합을 통해 집단을 분류하고 예측하는 분석 방법
- 오분류율이 최소가 되는 판별함수를 도출하고 판별 능력을 평가.
- 독립변수의 정규성, 등분산성을 가정함
- 판별함수 : 분류를 위한 기준으로 판별점수를 산출한다.
- 판별함수의 수=Min(집단의 수-1, 독립변수의 수)

#### 고급 분석 기법-시계열 분석
- 시계열 데이터는 시간의 영향을 받는 데이터다. 시계열 데이터는 일정한 시간 간격을 두고 관측되므로 시차가 동일하며 이론적으로 결측값이 없다.
- 정상성(Stationary) : 시점에 상관없이 일정한 시계열 데이터의 특성을 의미함. 대부분 시계열 자료는 정상성을 만족하지 않는 비정상 시계열이다. 이에 따라 아래의 조건을 만족하는 정상 시계열로 정상화하여 분석을 수행한다.
  - 평균이 시점에 의존하지 않는다. 즉, 모든 시점의 평균이 동일하다.
  - 분산이 시점에 의존하지 않는다. 즉, 모든 시점의 분산이 동일하다.
  - 공분산은 시차에만 의존하고 시점에는 의존하지 않는다. 즉, 시차가 같으면 공분산은 동일하다.
- 비정상성 확인
  - 자기상관함수(ACF : Autocorrelation Function) : 자체 시계열 데이터간의 선형 상관관계 함수.
  - 부분자기상관함수(PACF : Partial Autocorrelation Function) : 두 시점 사이에 영향을 주는 다른 요인을 제외한 자기상관함수
- 비정상 시계열의 정상화
  - 이상치가 있는 경우 이상치를 제거하거나 대체해 정상화함
  - 평균이 일정하지 않은 경우 차분(Difference)을 통해 정상화함
  - 분산이 일정하지 않은 경우 변환(Transformation)을 통해 정상화함


#### 시계열 모형(Time-Series Model)
1. 시계열 회귀분석
  - 회귀식 기반으로 시계열 자료를 분석하는 방법. 일반적인 회귀 모형과 같이 오차항에 대해 정규성, 등분산성, 독립성을 가정함
  - 선형다항식추세모형, 다항추세모형이라고도 함.

2. 분해법(Decomposition method) - 기출
  - 시계열 성분들이 결정적이고 서로 독립이라는 가정을 기반으로 성분을 분해하는 방법
  - 여러 성분들의 결합 방식에 따라 가법모형 또는 승법모형을 사용한다. 시간에 따라 계절성분의 진폭이 달라질 때 승법모형을 사용함
  - 시계열 성분 구분
    - 불규칙성분 : 규칙적이지 않고 예측이 불가한 랜덤 변동
    - 추세성분 : 지속적으로 증가하거나 감소하는 추세를 갖는 변동
    - 계절성분 : 계절 변화와 같은 주기적인 성분에 의한 변동
    - 순환성분 : 주기적인 변화를 가지나 주기가 긴 변동
  - 가법모형은 모든 성분을 더하고, 승법모형은 모든 성분은 곱하는 형태로 이루어짐

3. 이동평균법(Moving Average)
- 일정 기간의 관측치에 동일한 가중치를 부여하여 이동평균을 계산하는 방법. 계절성분과 불규칙 성분을 제거하는 특징을 가짐

4. 지수평활법(Exponential Smoothing)
- 모든 시점에 동일한 가중치를 부여하는 이동평균모형과 달리 최근 관측치에 더 높은 가중치를 부여하는 방법
- 최근 시점에 큰 가중치를 주고 과거 시점으로 갈수록 가중치를 지수적으로 줄여나간다.
- 선형 추세를 갖는 경우 이중지수평활법, 계절 추세를 갖는 경우 계절지수평활법을 사용함

5. 자기회귀 모형(AR : Autoregressive) : 관측치에 대해서 이전 값이 이후 값에 영향을 미치는 상황에 사용함
6. 이동평균 모형(MA : Moving Average) : 평균이 시간에 따라 변화하는 경향을 의미함 
7. 자기회귀 이동평균 모형(ARMA : Autoregressive Moving Average) : AR모형과 MA 모형을 결합한 형태.
8. 자기회귀 누적 이동평균 모형(ARIMA : Autoregressive Integrated Moving Average) : 비정상 시계열에 대해 d차로 차분 변환하는 과정을 포함한 ARMA 모형
9. 계절형 자기회귀 이동평균 모형(SARIMA : Seasonal ARIMA) : 시계열을 계절 성분을 포함하는 경우 사용하는 시계열 모형
10. 시계열모형 식별 방법 : 시계열 데이터의 ACF와 PACF 그래프를 이론적 그래프와 비교하여 차수를 식별한다.


#### 베이지안 기법
- 조건부 확률 : 특정 사건이 발생했다는 가정하에 다른 사건이 발생할 확률. 두 사건 A,B에 대하여 서로를 조건하는 조건부 확률은 다음과 같이 정의됨
  - 사건 A 조건하에 사건 B가 발생할 확률 : P(B|A)
  - 사건 B 조건하에 사건 A가 발생할 확률 : P(A|B)
- 베이즈 정리 : 표본이 특정 사건에 포함된다는 주장에 대한 신뢰도를 의미함. 베이즈 정리는 신규 데이터를 기반으로 베이지안 확률을 갱신하는 방법이다.


#### 나이브 베이즈 - 기출
- 베이즈 정리 기반의 지도 학습 분류 모델. 이론적으로 쉽고 산출 속도가 빠르다는 특징을 가진다.
- 종속변수를 추정하기 위해 모든 독립변수가 서로 동등하고 독립적으로 기여한다고 가정한다.
- 관측치가 종속변수의 각 범주에 속할 확률을 구하고 확률이 큰 범주에 할당한다.


#### 딥 러닝 분석
- 대용량 비정형 데이터 분석을 위한 인공신경망 기반 머신러닝 알고리즘
- 데이터 부족, 컴퓨팅 성능 한계 등 환경적 문제와 비선형 문제, 경사 소실 등 이론적 문제가 해결되면서 빠르게 발전해옴
- 주요 하이퍼 파라미터
  - learning rate : 파라미터의 업데이트 정도를 결정
  - 1 epoch : 모든 학습용 데이터가 한 번씩 forward pass와 backward pass를 진행
  - 1 iteration : 한 번의 forwatd pass와 backward pass르르 진행
  - mini-batch size : 1 iteration에 학습할 학습용 데이터의  샘플 수


#### 합성곱 신경망
- 이미지 처리에 특화된 딥러닝 알고리즘.
- 이미지의 특징을 추출하는 합성곱(Convolution)과 풀링(Pooling) 영역과 분류를 수행하는 완전연결신경망(Fully-connected neural network) 영역으로 구성됨
- 합성곱(Convolution) - 기출
  - 이미지 데이터로부터 특징을 추출하는 과정.
  - 필터를 이용해 유사한 이미지 영역을 강조하는 특정 맵을 출력함.
  - 특성 맵은 합성곱을 거치면서 사이즈가 점점 작아진다. 패딩(Padding)은 이미지 주변에 계산과 무관한 테두리를 추가하여 특성 맵의 사이즈를 조절한다.
- 풀링(Pooling)
  - 합성곱 과정을 거친 데이터를 요약한다.
  - 추출한 특징은 유지하면서 데이터 사이즈를 줄일 수 있다.
  - 학습 대상 파라미터 수를 줄이고 과적합을 방지하는 효과를 가진다.


#### 순환신경망(RNN : Recurrent neural network)
- 언어 데이터, 시계열 데이터와 같은 순차적인 데이터 학습에 특화된 알고리즘
- 과거의 학습을 현재 학습에 반영하는 순환구조를 가진다.
- 매 시점 데이터를 처리할 때 동일한 파라미터를 공유한다. 즉, 현 시점의 정보는 현 시점의 입력값과 이전 시점의 정보로 구성되어 전 시점에 걸쳐 파라미터를 공유하게 된다.
- 입력과 출력의 길이가 유연하기 때문에 다양한 모형을 설계할 수 있다.
- 장단기 메모리(LSTM : Long short term memory)
  - RNN의 단점을 보완. RNN은 시간을 거슬러 올라갈수록 경사소멸, 장기의존성 문제가 발생할 수 있음
  - LSTM은 순환구조에서 불필요한 정보를 삭제하거나 정보의 중요도에 따라 가중치를 조절한다.



#### 생성적 적대 신경망(GAN : Generative adversarial network)
- 진짜 같은 가짜를 만들도록 학습하는 생성자와 가짜와 진짜를 판별하도록 학습하는 구분자가 대립하여 서로의 성능을 개선하며 학습하는 알고리즘
- 생성자는 진짜 같은 데이터를 생성하므로 데이터 부족 문제, 불균형 문제를 해결하기 위한 방법으로 사용되기도 한다.


#### 비정형 데이터 분석-텍스트 마이닝
- 텍스트 데이터를 자연어 처리(NLP : Natural language precessing) 등의 방식으로 특징 추출, 요약, 분류 군집화 등 의미를 도출하는 분석 방법. 입력된 텍스트를 정형화하고 패턴을 추출하여 결과를 평가하거나 번역하는 과정을 거침
- 텍스트 마이닝 기능
  - 특징 추출 : 문서 내의 중요 정보, 원하는 정보 추출
  - 문서 요약 : 문서의 주요 정보를 유지하고 복잡도와 길이를 요약
  - 문서 분류 : 문서 내용을 분석해 정의된 카테고리로 분류
  - 문서 군집화 : 유사도를 기반으로 관련성 높은 문서끼리 군집화
- 텍스트 마이닝 절차
  1. 텍스트 수집 및 전처리 : 클렌징, 토큰화, 불용어 제거, 어간 추출, 표제어 추출 등의 과정을 거침
  2. 의미 추출 : 복잡한 문서 정보의 표현을 단순화 하여 의미있는 데이터로 변환함
  3. 패턴 분석 : 데이터 분석 및 시각화를 하는 단계
- 감정 분석 : 텍스트에 내재된 의견, 감성 등의 주관적인 정보를 분석하는 방법.
텍스트에서 긍/부정 여부를 판단하여 소비자 반응이나 여론 변화 등을 분석하는 목적으로 사용한다.


#### 소셜 네트워크 분석(SNA : Social network analysis)
- 개인, 집단, 사회의 관계를 네트워크 구조로 분석하고 시각화하는 방법.
- 네트워크는 노드와 엣지로 이루어지며, 이는 행렬 형태로도 표현할 수 있다.
- 분석 주요 속성
  - 명성(Prominence) : 권력 또는 책임을 가지고 있는 객체 확인
  - 응집력(Cohesion) : 객체 간 직접적 연결 존재 확인
  - 범위(Range) : 객체의 네트워크 규모
  - 중개(Brokerage) : 다른 네트워크와의 연결 정도
  - 구조적 등위성(Equivalence) : 한 네트워크의 구조적 지위와 역할이 동일한 객체들간의 관계
- 소셜 네트워크 분석 기준-밀도
  - 연결정도(Degree) : 노드 간의 총 연결 개수. 한 노드가 몇 개의 노드와 연결되어 있는지 정도
  - 포괄성(Indlusiveness) : 서로 연결된 노드 수. 연결되지 않은 노드를 제외한 노드 수
- 소셜 네트워크 분석 기준-중심성
  - 연결정도 중심성 : 직접 연결된 노드들의 합을 기반으로 측정
  - 근접 중심성 : 모든 노드로의 최소거리를 기반으로 측정
  - 매개 중심성 : 다른 노드를 사이의 위치하는 정도를 나타내는 지표
  - 위세 중심성 : 연결된 노드의 영향력에 가중치를 주어 측정


#### 앙상블 분석
- 분석 결과의 성능을 향상시키기 위해 다수의 모형에서 출력된 결과를 종합하여 하나의 최종 결과를 도출하는 방법
- 회귀 분석에 사용하는 경우 평균 등의 대푯값을 산출해 결과를 종합한다.
- 분류 분석의 경우 다수결 방식, 가중 다수결 방식 등을 활용해 최종 결과를 산출한다.
1. 배깅(Bagging
  - 부트스트랩 샘플링으로 추출한 여러 개의 표본에 각각 모형을 병렬적으로 학습하고 추출된 결과를 집계하는 앙상블 기법. 성능 향상에 효과적이며 데이터의 사이즈가 작거나 결측값이 있는 경우 유리한 방법임
2. 랜덤포레스트(Random Forest) -기출
- 의사결정나무 기반의 앙상블 알고리즘. 기본 배깅에 변수를 랜덤으로 선택하는 Feature Bagging 과정을 추가한 방법.
- 랜덤하게 변수를 선택하여 동일한 트리가 생성되는 것을 방지하고 변수가 많은 경우 별도의 변수 제거 없이 분석이 가능하다.
- 예측 편향을 줄이고 과적합을 방지할 수 있으며 이상치에 영향을 적게 받는다.


#### 부스팅(Boosting)
- 예측력이 약한 모형을 순차적으로 결합하여 예측력이 강한 모형을 만드는 앙상블 기법.
- 순차적으로 학습하며 데이터의 가중치를 재조정함.
- 잘못 분류한 데이터는 높은 가중치를 부여하고 잘 분류한 데이터는 낮은 가중치를 부여한다.
1. AdaBoost(Adaptive boosting)
- 약한 모형을 하나씩 순차적으로 학습함. 먼저 학습한 모형이 잘못 분류한 표본에 높은 가중치를 부여하고 다음 모형은 높은 가중치가 부여된 표본을 잘 분류할 수 있도록 한다.
- 여러 모형을 순차적으로 학습하고 각각의 결과를 종합하여 강한 모형을 생성한다.

2. GBM(Gradient boosting machine)
- Adaboost와 유사하나 가중치를 조정할 때 경사하강법을 이용하여 최적화된 결과를 얻는다.
  - 경사하강법 : 잔차(오류)를 최소화하는 방향으로 가중치를 재조정하는 방식
- GBM은 대표적인 탐욕 알고리즘임. 과적합될 확률이 높고 학습시간이 길다는 단점이 있다.
- XGBoost : GBM의 단점을 보완하기 위해 시스템을 최적화하고 알고리즘을 고도화한다.시스템 최적화 관점으로 병렬화, 가지치기를 적용한다.
- LightBGM : 트리 분할에 Level-wise 방식을 사용하는 GBM, XGBoost와 달리 Leaf-wise 방식을 사용한다.
  - Level-wise 방식 : 균형 트리 분할 방식. 최대한 균형잡힌 트리를 유지하면서 분할하기에 깊이가 최소화됨
  - Leaf-wise 방식 : 최대 손실을 갖는 리프 노드를 지속 분할하여 깊고 비대칭적인 트리를 생성함

#### 비모수 통계
- 모집단의 모수를 추정하기 위한 통계적 검정 방법은 모수 통계와 비모수 통계로 구분된다.
- 모수 통계 : 모집단의 분포를 가정하고 분포를 기반으로 검정
- 비모수 통계 : 모집단의 분포를 가정하지 않고 빈도, 부호, 순위 등 명목척도 또는 서열척도를 활용해 검정
- 장점 : 모집에 분포에 대한 가정이 없음, 통계량 산식이 단순하고 직관적임
- 단점 : 모집단 분포 가정을 만족하면 효율이 떨어짐, 표본 사이즈가 큰 경우 계산량이 과도함


#### 부호검정(Sign test)
- 부호만을 기준으로 모집단의 중앙값을 검정하는 비모수적 통계방법
- 이론적인 분포를 가정하지 않으나 분포의 연속성, 독립성을 가정함



#### 윌콕슨 부호 순위 검정(Wilcoxon signed rank test)
- 부호와 상대적 크기를 고려해 중앙값을 검정하는 비모수적 통계방법
- 분포의 연속성, 독립성, 대칭성을 가정함
(기존 부호검정의 단점을 보완)

#### 만-위트리 U 검정(Mann-Whitney U test)
- 두 모집단 간의 중앙값 위치를 비교하는 비모수적 방법
- 분포의 연속성, 독립성, 대칭성을 가정함

#### 크루스칼-왈리스 검정(Kruskal-Wallis test)
- 세 개 이상 집단의 부포를 비교하는 검정 방법
- 순위합 검정법의 하나로 모든 집단의 혼합 표본에서 순위 합을 구하여 검정 통계량을 계산한다.
- 각 표본의 중앙값은 다르나 동일한 형태의 분포를 가진다 가정함

#### 런 검정(Run test)
- 각 표본이 서로 독립적인지 검정하는 검정 방법
- 어떤 패턴이나 경향 없이 랜덤하게 나타나는지 검정하고 표본을 배타적인 2개의 집단으로 구분한다.

#### 스피어만 순위 상관계수(Spearman rank correlation coefficinet)
- 두 변수의 순위 값을 기반으로 산출한 상관관계를 평가하는 비모수 척도
- 두 변수 간의 스피어만 상관계수는 순위 값을 기반으로 산출한 피어슨 상관계수와 같다.
- 선형적 관계만을 평가하는 피어슨 상관계수와 달리 스피어만 상관계수는 단조적 관계를 평가한다.


#### 오답 정리
- 회귀방정식의 결정계수 값은 상관계수의 제곱이다.
- 회귀방정식의 회귀계수는 y=ax+b에서 a를 담당함
  - 이 때, 상관계수와 회귀 계수의 차이는 회귀계수=상관계수*y표준편차/x표준편차
- 의사결정나무는 수치자료와 범주자료 모두 적용할 수 있다.
- 의사결정나무 분석에서 분류 기분 변수 선택에 사용되는 기준으로는 지니지수, 엔트로피 지수, 카이제곱 통계량의 p값이 있음
- 최소제곱법은 회귀 계수를 추정할 때 사용한다. 측정값을 기초로 해서 제곱합을 만들고, 그것을 최소로 하는 값을 구하며 최소자승법이라고도 부른다.
- K-평균 군집은 개체와 개체들이 속하는 군집의 중심 간 거리가 가장 작은 군집방법이다.
- 덴드로그램은 계층적 군집 분석의 결과를 표현하는 그림이다.
- 분할표 분석에서 오즈비는 각 집단의 Odds의 비율이다. 집단별로 구할 수 있는 값은 Odds이다.
- 정상 시계열의 공분산은 시차에만 의존하고 시점 자체에는 의존하지 않는다.
- LSTM은 RNN의 주요 모델 중 하나로 장기 의존성 문제를 해결한다. 패딩은 CNN에서 특성 맵의 사이즈를 조절하는 방법이다.
- 인공신경망의 은닉 노드가 많은 경우 과적합 가능성이 높고 일반화가 어렵다. 레이어 수가 많으면 기울기 소실의 가능성이 높다. 노드 수가 적으면 과소적합의 가능성이 높다.
- 패딩은 이미지 주변에 계산과 무관한 테두리를 추가하여 특성 맵의 사이즈를 조정한다.
- 스피어만 상관계수는 두 변수의 순위 간 통계적 의존성을 측정하는 비모수적인 척도이다. 두 변수의 순위값 사이의 피어슨 상관 계수와 같으며, 단순 선형 관계 평가가 아닌 비선형 또는 단조적 관계를 평가한다.